<source>
  @type forward
  port 24225
  bind 127.0.0.1
</source>	

# Expose prometheus metrics on 24231 (default)
<source>
  @type prometheus
</source>

# input plugin that collects metrics from MonitorAgent
<source>
  @type prometheus_monitor
  <labels>
    host ${hostname}
  </labels>
</source>

<filter billing>
  @type prometheus
  <metric>
    name billing_ingester_events_total
    type counter
    desc Number and type of billing events
    <labels>
      status success
      amount_type ${amount_type}
    </labels>
  </metric>
  <metric>
    name billing_ingester_amounts_total
    type counter
    desc Number and type of billing amounts
    key amount_value
    <labels>
      status success
      amount_type ${amount_type}
    </labels>
  </metric>
</filter>

<match billing>
  @type bigquery
  # We should move to loads and increase flush intervals
  # once we reach a considerable event rate
  method insert

  # buffering
  buffer_type file
  # 'fluent' user needs to have write access.
  # See https://github.com/weaveworks/service-conf/issues/1283
  buffer_path /tmp/billing.*.buffer
  flush_interval 1
  try_flush_interval 0.1
  queued_chunk_flush_interval 0.01
  flush_at_shutdown true
  retry_wait 1s
  max_retry_wait 10s # default is infinite
  disable_retry_limit false
  # Buffer up to 5GB worth of data (1MB*5000).
  # Assuming a maximum of 1KB per event that should be around 5 million events.
  # At a rate of 100 events/second this should give us ~7 hours of buffering.
  #
  # You should set the buffer_chunk_limit to lte than the
  # kinesis 1mb record limit, since we ship a chunk at once.
  buffer_chunk_limit 1m
  buffer_queue_limit 5000

  auth_method json_key
  json_key /bigquery/secret/key.json

  project weaveworks-bi
  dataset billing_dev
  table events
  schema_path /bigquery/schema_events.json

  insert_id_field unique_key
  add_insert_timestamp received_at
</match>
