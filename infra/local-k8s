#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail

function usage {
	echo "usage: $(basename $0) up [host_ip] / down" 1>&2
	exit 1
}

if [ $# -lt 1 ]
then
	usage
fi

WHAT=$1

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
LOCAL_K8S_DIR="${SCRIPT_DIR}/../k8s/local"
LOCAL_MASTER_K8S_DIR="${LOCAL_K8S_DIR}/master"
LOCAL_PKI_K8S_DIR="${LOCAL_MASTER_K8S_DIR}/pki"

function stop_container {
	docker stop $1 > /dev/null 2>&1 || true
	docker rm -v $1 > /dev/null 2>&1 || true
}

function write_kubeconfig {
	local k8s_ip="$1"
	cat <<EOF >${SCRIPT_DIR}/local/kubeconfig
apiVersion: v1
clusters:
- cluster:
    server: http://${k8s_ip}:8080
  name: local
contexts:
- context:
    cluster: local
    user: ""
  name: local
current-context: local
kind: Config
preferences: {}
users: []
EOF
}

function remove_kubeconfig {
	rm -f ${SCRIPT_DIR}/local/kubeconfig
}

function tear_down {
	for c in $(docker ps -q -f 'name=k8s*')
	do
		stop_container $c
	done
	stop_container local_k8s_proxy
	stop_container local_k8s_kubelet
	stop_container etcd
	remove_kubeconfig
}

# https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md
function stand_up {
	if ! command -v kubectl > /dev/null 2>&1
	then
		echo "kubectl not found" >&2
		exit 1
	fi

	if [ $# -eq 1 ]
	then
	    # Use a specific host
	    local k8s_ip="$1"
	else
	    case "$(uname -s)" in
		"Darwin")
		    local machine_name=$(docker-machine active)
		    echo "Using docker-machine ${machine_name}"
		    k8s_ip=$(docker-machine ip ${machine_name})
		    ;;
		*)
		    local k8s_ip="localhost"
		    ;;
	    esac
	fi

	echo "Starting etcd"
	docker run \
		--name etcd \
		--net=host \
		-d gcr.io/google_containers/etcd:2.0.12 \
		/usr/local/bin/etcd --addr=127.0.0.1:4001 --bind-addr=0.0.0.0:4001 --data-dir=/var/etcd/data

	sed "s%\$PKI_HOST_PATH%${LOCAL_PKI_K8S_DIR}%g" "${LOCAL_MASTER_K8S_DIR}"/master.json.in > "${LOCAL_MASTER_K8S_DIR}"/master.json

	# kubelet requires --docker-endpoint=$DOCKER_HOST, to make it talk to Weave.
	# But it doesn't work due to https://github.com/weaveworks/weave/issues/1600

	# HACK: Kubelet is run in the host mount namespace (using nsenter) to
	#       workaround https://github.com/kubernetes/kubernetes/issues/18239
	#       similarly to what is proposed at
	#       https://github.com/kubernetes/kubernetes/pull/19069
	#
	#       We also provide a special PATH (in which directories are
	#       prepended by '.' which still lives in the container's rootfs) so
	#       that the executables in the image can still be accessed.
	#
	#       The nsenter/PATH hack can be removed when we switch to Docker >=
	#       1.10 which releases support for volume mount propagation flags
	#       (PR https://github.com/docker/docker/pull/17034 ) allowing to
	#       mount /var/lib/hyperkube in 'shared' propagation mode.
	#
	#       Lengthy explanation: Kubelet expects the mounts under
	#       /var/lib/kubeletet/pods/*/volumes to be viewable by the
	#       containers it creates (Kubelet places Secret Volumes inside
	#       tmpfs mounts, to support establishing secret size limits I
	#       believe). Containers can see access those mounts when Kubelet is
	#       running in the host, but this won't work in general when running
	#       Kubelet in a container since mount events may not propagate down
	#       to the container (Docker >= 1.9 sets the root filesystem of the
	#       containers as MS_PRIVATE and only certain host systems, like the
	#       systemd-based set the hosts root as MS_SHARED). Using nsenter
	#       guarantees that kubelet runs in the host mount namespace,
	#       circumventing the problem. See
	#       https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt
	echo "Starting kubelet"
	docker run \
		--name local_k8s_kubelet \
		--net=host \
		--pid=host \
		--privileged=true \
		-e 'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:./usr/local/sbin:./usr/local/bin:./usr/sbin:./usr/bin:./sbin:./bin' \
		-d gcr.io/google_containers/hyperkube-amd64:v1.2.3 \
		nsenter --target=1 --wd=. --mount -- ./hyperkube kubelet --hostname-override="127.0.0.1" --address="0.0.0.0" --api-servers=http://localhost:8080 --config="${LOCAL_MASTER_K8S_DIR}" --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --allow-privileged

	docker run --name local_k8s_proxy -d --net=host --privileged gcr.io/google_containers/hyperkube-amd64:v1.2.3 /hyperkube proxy --master=http://127.0.0.1:8080 --v=2

	while ! kubectl --server=http://${k8s_ip}:8080 get svc kubernetes >/dev/null 2>&1
	do
		echo -n "."
		sleep 1
	done
	echo

	echo "Starting other components"
	kubectl --server=http://${k8s_ip}:8080 create -f ${LOCAL_K8S_DIR}/skydns
	kubectl --server=http://${k8s_ip}:8080 create -f ${LOCAL_K8S_DIR}/scope

	write_kubeconfig ${k8s_ip}

	echo ""
	echo "Kubernetes is running locally."
	echo ""
	echo " kubectl --kubeconfig=$(python -c "import os.path; print os.path.relpath('${SCRIPT_DIR}/local/kubeconfig', '$(pwd)')") get pods"
	echo ""
	echo "You may now deploy the application components."
	echo ""
}

function generate_certificates {
	echo "Generating certificates"
	if ! [ -d "${LOCAL_PKI_K8S_DIR}" ]
	then
		curl -s -L https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/saltbase/salt/generate-cert/make-ca-cert.sh -o /tmp/make-ca-cert.sh
		chmod +x /tmp/make-ca-cert.sh
		CERT_GROUP=`id -g` CERT_DIR="$LOCAL_PKI_K8S_DIR" /tmp/make-ca-cert.sh 10.0.0.1 DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster.local
	fi
}

case $WHAT in
up)
	tear_down
	generate_certificates
	if [ $# -gt 1 ]
	then
	    stand_up $2
	else
	    stand_up
	fi
	;;

down)
	tear_down
	;;

*)
	echo "Unknown command" 1>&2
	usage
	;;
esac
